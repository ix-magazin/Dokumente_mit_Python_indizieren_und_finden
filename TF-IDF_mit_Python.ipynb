{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a508d80-cdf6-4301-a22f-a17158ae69e5",
   "metadata": {},
   "source": [
    "# TF-IDF Based Search\n",
    "\n",
    "## I. Simple Example\n",
    "\n",
    "\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9e7866-1258-45c1-b9f6-0305372dbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4dc4c-c35a-4eea-8129-22156227323d",
   "metadata": {},
   "source": [
    "### Prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1b0924-98af-4ceb-9886-574cccba95d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sie trinkt kaffee und sie liest bücher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sie trinkt tee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sie kauft bücher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    corpus\n",
       "id                                        \n",
       "0   sie trinkt kaffee und sie liest bücher\n",
       "1                           sie trinkt tee\n",
       "2                         sie kauft bücher"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'sie trinkt kaffee und sie liest bücher'\n",
    "b = 'sie trinkt tee'\n",
    "c = 'sie kauft bücher'\n",
    "\n",
    "df = pd.DataFrame({'corpus': [a,b,c]})\n",
    "df.index.name='id'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1aee36-11fc-4d4d-9290-dd433a63c4bd",
   "metadata": {},
   "source": [
    "### Prepare tfidf-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6c94db-5d19-4aa3-bbd1-180839903d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bücher</th>\n",
       "      <th>kaffee</th>\n",
       "      <th>kauft</th>\n",
       "      <th>liest</th>\n",
       "      <th>sie</th>\n",
       "      <th>tee</th>\n",
       "      <th>trinkt</th>\n",
       "      <th>und</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bücher    kaffee     kauft     liest  sie       tee    trinkt       und\n",
       "id                                                                           \n",
       "0   1.287682  1.693147  0.000000  1.693147  2.0  0.000000  1.287682  1.693147\n",
       "1   0.000000  0.000000  0.000000  0.000000  1.0  1.693147  1.287682  0.000000\n",
       "2   1.287682  0.000000  1.693147  0.000000  1.0  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df['corpus'].tolist()\n",
    "model = TfidfVectorizer(norm=None)\n",
    "model.fit(data)\n",
    "X = model.transform(data)\n",
    "\n",
    "dcount = pd.DataFrame(X.toarray(), columns = model.get_feature_names_out())\n",
    "dcount.index.name='id'\n",
    "dcount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66e92e-97dc-418e-af34-e4e2e260447b",
   "metadata": {},
   "source": [
    "### Prepare query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45601950-3594-4cee-ac4a-6d1d46810f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['er trinkt kaffee und tee']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = [\"er trinkt kaffee und tee\"]\n",
    "#query = [\"er kauft limonade\"]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3e0bb4-e63d-4533-9238-6bf5d4699927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bücher</th>\n",
       "      <th>kaffee</th>\n",
       "      <th>kauft</th>\n",
       "      <th>liest</th>\n",
       "      <th>sie</th>\n",
       "      <th>tee</th>\n",
       "      <th>trinkt</th>\n",
       "      <th>und</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bücher    kaffee  kauft  liest  sie       tee    trinkt       und\n",
       "0     0.0  1.693147    0.0    0.0  0.0  1.693147  1.287682  1.693147"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xq = model.transform(query)\n",
    "print(type(Xq))\n",
    "\n",
    "#  columns = model.get_feature_names_out()\n",
    "pd.DataFrame(columns=dcount.columns, data = Xq.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2419a281-901e-4803-8a0c-7539026c6697",
   "metadata": {},
   "source": [
    "dcount = pd.DataFrame(Xq.toarray(), columns = model.get_feature_names_out())\n",
    "dcount.index.name='id'\n",
    "dcount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463a36e-47a9-429d-af62-23fd5a79c837",
   "metadata": {},
   "source": [
    "### Search Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37f48d6-adc3-4578-a763-3ae8b9601250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xa shape:  (3, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.28768207, 1.69314718, 0.        , 1.69314718, 2.        ,\n",
       "        0.        , 1.28768207, 1.69314718],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.69314718, 1.28768207, 0.        ],\n",
       "       [1.28768207, 0.        , 1.69314718, 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Xqa shape:  (1, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.69314718, 0.        , 0.        , 0.        ,\n",
       "        1.69314718, 1.28768207, 1.69314718]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xa = X.toarray()\n",
    "Xqa = Xq.toarray()\n",
    "\n",
    "print('Xa shape: ', Xa.shape)\n",
    "display(Xa)\n",
    "print()\n",
    "print('Xqa shape: ', Xqa.shape)\n",
    "display(Xqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2748e5bc-7a5a-4d45-872f-4ab4847f665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.39161987, 4.52487249, 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Xqa @ Xa.T\n",
    "print('result: ')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0821d4c-0935-4150-9334-68f641aaef2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sie trinkt kaffee und sie liest bücher</td>\n",
       "      <td>7.391620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sie trinkt tee</td>\n",
       "      <td>4.524872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sie kauft bücher</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    corpus    result\n",
       "id                                                  \n",
       "0   sie trinkt kaffee und sie liest bücher  7.391620\n",
       "1                           sie trinkt tee  4.524872\n",
       "2                         sie kauft bücher  0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['result'] = result[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82272c-dd7b-4c87-954c-18df9abb2fad",
   "metadata": {},
   "source": [
    "# II. Simple Search Engine\n",
    "\n",
    "## Part 1: Prepare Data\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be01558e-649e-4281-b900-f335966d21b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sie trinkt kaffee und sie liest bücher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sie trinkt tee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sie kauft bücher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    corpus\n",
       "id                                        \n",
       "0   sie trinkt kaffee und sie liest bücher\n",
       "1                           sie trinkt tee\n",
       "2                         sie kauft bücher"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "a = 'sie trinkt kaffee und sie liest bücher'\n",
    "b = 'sie trinkt tee'\n",
    "c = 'sie kauft bücher'\n",
    "\n",
    "df = pd.DataFrame({'corpus': [a,b,c]})\n",
    "df.index.name='id'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47389620-f62b-4295-8d1d-0fa2435f5fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c012476-5c1e-4ab8-ab45-b2e3ac0a9b74",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Crawler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf131c5-7688-417c-8a59-efb72aef68f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94c06eb0-421a-4782-b7b8-44ba8875179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import wikipediaapi # vers <0.6; otherwise add user agent: https://pypi.org/project/Wikipedia-API/\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa26f125-6088-45be-b3c2-02f7f5240deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler data\n",
    "\n",
    "LANG = 'en' # 'de'\n",
    "# read data from internet\n",
    "READDATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cdc54e6-84ca-4d66-9d87-a4b05a371186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LANG == 'en':\n",
    "    # repositories\n",
    "    #fn_crawler_datapq = 'data/corpuspq.gzip'\n",
    "    fn_corpus = 'data/corpus.csv'\n",
    "    articles = ['Desk pad','Data Science','Artificial intelligence', 'Somaliland', \n",
    "                 'Natural language processing', 'Arabian Sea', 'Suez Canal', \n",
    "                 'Statistics','Dependent and independent variables', 'Gulf of Aden', \n",
    "                 'Machine Learning','European Central Bank','Bank','Financial technology', \n",
    "                 'International Monetary Fund','Basketball','Swimming','Tennis']\n",
    "\n",
    "elif LANG == 'de':\n",
    "    # repositories\n",
    "    #fn_crawler_datapq = 'data/corpuspq_de.gzip'\n",
    "    fn_corpus = 'data/corpus_de.csv'\n",
    "    articles_de = ['Schreibunterlage', 'Data Science', 'Künstliche Intelligenz', 'Somaliland', \n",
    "               'Verarbeitung natürlicher Sprache', 'Arabisches Meer', 'Suezkanal', \n",
    "               'Statistik', 'Abhängige und unabhängige Variable', 'Golf von Aden',\n",
    "               'Maschinelles Lernen', 'Europäische Zentralbank', 'Bank', \n",
    "               'Finanztechnologie', 'Internationaler Währungsfonds', 'Basketball', \n",
    "               'Schwimmen beim Menschen', 'Tennis']\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia(LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb85f42e-56da-431d-adb8-473884deec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    \"\"\"\n",
    "    remove non latin characters; remove \"\\n\"\n",
    "    input: string\n",
    "    return: cleaned string\n",
    "    \"\"\"\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    pattern = r'[^\\u0020-\\u017F]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1234d02-f88b-452a-a110-7227d0a76fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# crawler\n",
    "\n",
    "wiki_lst=[]\n",
    "title_lst=[]\n",
    "\n",
    "if READDATA:\n",
    "    for i, article in enumerate(articles):\n",
    "        print(f'{i}: {article}')\n",
    "        page = wiki.page(article)\n",
    "        text = filter_text(page.text)\n",
    "        wiki_lst.append(text)\n",
    "        title_lst.append(page.title)\n",
    "\n",
    "    pd.DataFrame({'article': wiki_lst, 'title': title_lst}).to_csv(fn_corpus,  index = None)  \n",
    "    #df.to_parquet(crawler_datapq, engine = 'fastparquet', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8650c3-1d35-4ee1-bda6-b33f80b2f0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d732fc49-8ae1-4e8d-9e87-8cb1cbf0865f",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35aa6edc-4cae-4ed6-9c30-058d5280324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187f832-5752-4458-aa06-12648c8abbc4",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4f352b-ddfa-43ac-a05b-abf2acf85bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'en'\n",
    "\n",
    "if LANG == 'en':\n",
    "    # repositories\n",
    "    #crawler_data = 'data/corpus_raw.csv'\n",
    "    #fn_crawler_datapq = 'data/corpuspq.gzip'\n",
    "    fn_corpus = 'data/corpus.csv'\n",
    "\n",
    "elif LANG == 'de':\n",
    "    # repositories\n",
    "    #crawler_data = 'data/corpus_raw.csv'\n",
    "    #fn_crawler_datapq = 'data/corpuspq_de.gzip'\n",
    "    fn_corpus = 'data/corpus_de.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9dca84-5b49-4049-a529-6dfc435e85a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A desk pad or blotter is a table protector use...</td>\n",
       "      <td>Desk pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data science is an interdisciplinary academic ...</td>\n",
       "      <td>Data science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial intelligence (AI) is intelligencepe...</td>\n",
       "      <td>Artificial intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Somaliland, officially the Republic of Somalil...</td>\n",
       "      <td>Somaliland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natural language processing (NLP) is an interd...</td>\n",
       "      <td>Natural language processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Arabian Sea (Arabic:  , romanized: Al-Bahr...</td>\n",
       "      <td>Arabian Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Suez Canal (Egyptian Arabic:  , Qanāt el S...</td>\n",
       "      <td>Suez Canal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Statistics (from German: Statistik, orig. \"des...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dependent and independent variables are variab...</td>\n",
       "      <td>Dependent and independent variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Gulf of Aden (Arabic:  , Somali: Gacanka C...</td>\n",
       "      <td>Gulf of Aden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Machine learning (ML) is a field devoted to un...</td>\n",
       "      <td>Machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The European Central Bank (ECB) is the prime c...</td>\n",
       "      <td>European Central Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A bank is a financial institution that accepts...</td>\n",
       "      <td>Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fintech, a portmanteau of \"financial technolog...</td>\n",
       "      <td>Fintech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The International Monetary Fund (IMF) is a maj...</td>\n",
       "      <td>International Monetary Fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Basketball is a team sport in which two teams,...</td>\n",
       "      <td>Basketball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Swimming is the self-propulsion of a person th...</td>\n",
       "      <td>Swimming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tennis is a racket sport that is played either...</td>\n",
       "      <td>Tennis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              article  \\\n",
       "0   A desk pad or blotter is a table protector use...   \n",
       "1   Data science is an interdisciplinary academic ...   \n",
       "2   Artificial intelligence (AI) is intelligencepe...   \n",
       "3   Somaliland, officially the Republic of Somalil...   \n",
       "4   Natural language processing (NLP) is an interd...   \n",
       "5   The Arabian Sea (Arabic:  , romanized: Al-Bahr...   \n",
       "6   The Suez Canal (Egyptian Arabic:  , Qanāt el S...   \n",
       "7   Statistics (from German: Statistik, orig. \"des...   \n",
       "8   Dependent and independent variables are variab...   \n",
       "9   The Gulf of Aden (Arabic:  , Somali: Gacanka C...   \n",
       "10  Machine learning (ML) is a field devoted to un...   \n",
       "11  The European Central Bank (ECB) is the prime c...   \n",
       "12  A bank is a financial institution that accepts...   \n",
       "13  Fintech, a portmanteau of \"financial technolog...   \n",
       "14  The International Monetary Fund (IMF) is a maj...   \n",
       "15  Basketball is a team sport in which two teams,...   \n",
       "16  Swimming is the self-propulsion of a person th...   \n",
       "17  Tennis is a racket sport that is played either...   \n",
       "\n",
       "                                  title  \n",
       "0                              Desk pad  \n",
       "1                          Data science  \n",
       "2               Artificial intelligence  \n",
       "3                            Somaliland  \n",
       "4           Natural language processing  \n",
       "5                           Arabian Sea  \n",
       "6                            Suez Canal  \n",
       "7                            Statistics  \n",
       "8   Dependent and independent variables  \n",
       "9                          Gulf of Aden  \n",
       "10                     Machine learning  \n",
       "11                European Central Bank  \n",
       "12                                 Bank  \n",
       "13                              Fintech  \n",
       "14          International Monetary Fund  \n",
       "15                           Basketball  \n",
       "16                             Swimming  \n",
       "17                               Tennis  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(fn_corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20382415-40b4-4a40-88e5-278f5fe53ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7cec4-e460-4bb5-9b16-85cc85b64d84",
   "metadata": {},
   "source": [
    "### Optional: Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e26c64b-6b86-4511-abf8-2befbaf48962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 19:52:25.589430: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-19 19:52:26.477256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-19 19:52:26.477392: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-19 19:52:26.477405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-11-19 19:52:27.285882: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-11-19 19:52:27.285918: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-11-19 19:52:27.285955: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rp5): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "TOKENIZER = True\n",
    "FILTER = True\n",
    "\n",
    "if TOKENIZER: \n",
    "    # keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load('en_core_web_sm',  disable=[\"parser\", \"ner\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361eaef4-52ca-4157-a90f-2e1bd021f96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 1.1 s, total: 14.6 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 15 sec for 45'000 words \n",
    "\n",
    "if TOKENIZER:\n",
    "    corpus['lemma'] = corpus['article'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e434f33-0bcf-4a3b-aa29-64dcd87b6e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>title</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A desk pad or blotter is a table protector use...</td>\n",
       "      <td>Desk pad</td>\n",
       "      <td>a desk pad or blotter be a table protector use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data science is an interdisciplinary academic ...</td>\n",
       "      <td>Data science</td>\n",
       "      <td>datum science be an interdisciplinary academic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial intelligence (AI) is intelligencepe...</td>\n",
       "      <td>Artificial intelligence</td>\n",
       "      <td>artificial intelligence ( AI ) be intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Somaliland, officially the Republic of Somalil...</td>\n",
       "      <td>Somaliland</td>\n",
       "      <td>Somaliland , officially the Republic of Somali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natural language processing (NLP) is an interd...</td>\n",
       "      <td>Natural language processing</td>\n",
       "      <td>natural language processing ( NLP ) be an inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Arabian Sea (Arabic:  , romanized: Al-Bahr...</td>\n",
       "      <td>Arabian Sea</td>\n",
       "      <td>the Arabian Sea ( Arabic :   , romanize : Al -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Suez Canal (Egyptian Arabic:  , Qanāt el S...</td>\n",
       "      <td>Suez Canal</td>\n",
       "      <td>the Suez Canal ( Egyptian Arabic :   , Qanāt e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Statistics (from German: Statistik, orig. \"des...</td>\n",
       "      <td>Statistics</td>\n",
       "      <td>statistic ( from german : Statistik , orig . \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dependent and independent variables are variab...</td>\n",
       "      <td>Dependent and independent variables</td>\n",
       "      <td>dependent and independent variable be variable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Gulf of Aden (Arabic:  , Somali: Gacanka C...</td>\n",
       "      <td>Gulf of Aden</td>\n",
       "      <td>the Gulf of Aden ( Arabic :   , Somali : Gacan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Machine learning (ML) is a field devoted to un...</td>\n",
       "      <td>Machine learning</td>\n",
       "      <td>machine learning ( ML ) be a field devote to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The European Central Bank (ECB) is the prime c...</td>\n",
       "      <td>European Central Bank</td>\n",
       "      <td>the European Central Bank ( ECB ) be the prime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A bank is a financial institution that accepts...</td>\n",
       "      <td>Bank</td>\n",
       "      <td>a bank be a financial institution that accept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fintech, a portmanteau of \"financial technolog...</td>\n",
       "      <td>Fintech</td>\n",
       "      <td>Fintech , a portmanteau of \" financial technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The International Monetary Fund (IMF) is a maj...</td>\n",
       "      <td>International Monetary Fund</td>\n",
       "      <td>the International Monetary Fund ( IMF ) be a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Basketball is a team sport in which two teams,...</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>basketball be a team sport in which two team ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Swimming is the self-propulsion of a person th...</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>swimming be the self - propulsion of a person ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tennis is a racket sport that is played either...</td>\n",
       "      <td>Tennis</td>\n",
       "      <td>Tennis be a racket sport that be play either i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              article  \\\n",
       "0   A desk pad or blotter is a table protector use...   \n",
       "1   Data science is an interdisciplinary academic ...   \n",
       "2   Artificial intelligence (AI) is intelligencepe...   \n",
       "3   Somaliland, officially the Republic of Somalil...   \n",
       "4   Natural language processing (NLP) is an interd...   \n",
       "5   The Arabian Sea (Arabic:  , romanized: Al-Bahr...   \n",
       "6   The Suez Canal (Egyptian Arabic:  , Qanāt el S...   \n",
       "7   Statistics (from German: Statistik, orig. \"des...   \n",
       "8   Dependent and independent variables are variab...   \n",
       "9   The Gulf of Aden (Arabic:  , Somali: Gacanka C...   \n",
       "10  Machine learning (ML) is a field devoted to un...   \n",
       "11  The European Central Bank (ECB) is the prime c...   \n",
       "12  A bank is a financial institution that accepts...   \n",
       "13  Fintech, a portmanteau of \"financial technolog...   \n",
       "14  The International Monetary Fund (IMF) is a maj...   \n",
       "15  Basketball is a team sport in which two teams,...   \n",
       "16  Swimming is the self-propulsion of a person th...   \n",
       "17  Tennis is a racket sport that is played either...   \n",
       "\n",
       "                                  title  \\\n",
       "0                              Desk pad   \n",
       "1                          Data science   \n",
       "2               Artificial intelligence   \n",
       "3                            Somaliland   \n",
       "4           Natural language processing   \n",
       "5                           Arabian Sea   \n",
       "6                            Suez Canal   \n",
       "7                            Statistics   \n",
       "8   Dependent and independent variables   \n",
       "9                          Gulf of Aden   \n",
       "10                     Machine learning   \n",
       "11                European Central Bank   \n",
       "12                                 Bank   \n",
       "13                              Fintech   \n",
       "14          International Monetary Fund   \n",
       "15                           Basketball   \n",
       "16                             Swimming   \n",
       "17                               Tennis   \n",
       "\n",
       "                                                lemma  \n",
       "0   a desk pad or blotter be a table protector use...  \n",
       "1   datum science be an interdisciplinary academic...  \n",
       "2   artificial intelligence ( AI ) be intelligence...  \n",
       "3   Somaliland , officially the Republic of Somali...  \n",
       "4   natural language processing ( NLP ) be an inte...  \n",
       "5   the Arabian Sea ( Arabic :   , romanize : Al -...  \n",
       "6   the Suez Canal ( Egyptian Arabic :   , Qanāt e...  \n",
       "7   statistic ( from german : Statistik , orig . \"...  \n",
       "8   dependent and independent variable be variable...  \n",
       "9   the Gulf of Aden ( Arabic :   , Somali : Gacan...  \n",
       "10  machine learning ( ML ) be a field devote to u...  \n",
       "11  the European Central Bank ( ECB ) be the prime...  \n",
       "12  a bank be a financial institution that accept ...  \n",
       "13  Fintech , a portmanteau of \" financial technol...  \n",
       "14  the International Monetary Fund ( IMF ) be a m...  \n",
       "15  basketball be a team sport in which two team ,...  \n",
       "16  swimming be the self - propulsion of a person ...  \n",
       "17  Tennis be a racket sport that be play either i...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799fbe43-831f-45d1-b43a-a5a30bc28a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd0f45e-17cc-45c7-9b71-c37f707d565f",
   "metadata": {},
   "source": [
    "### Optional: Filter\n",
    "\n",
    "* remove numbers (only numbers, not numbers combined with letters)\n",
    "\n",
    "Token shorter than 3 letters will be removed by tf-idf method below, including punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fa6d7a2-4a37-4f68-9dc0-3d8089223c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>title</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A desk pad or blotter is a table protector use...</td>\n",
       "      <td>Desk pad</td>\n",
       "      <td>a desk pad or blotter be a table protector use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data science is an interdisciplinary academic ...</td>\n",
       "      <td>Data science</td>\n",
       "      <td>datum science be an interdisciplinary academic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial intelligence (AI) is intelligencepe...</td>\n",
       "      <td>Artificial intelligence</td>\n",
       "      <td>artificial intelligence ( AI ) be intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Somaliland, officially the Republic of Somalil...</td>\n",
       "      <td>Somaliland</td>\n",
       "      <td>Somaliland , officially the Republic of Somali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natural language processing (NLP) is an interd...</td>\n",
       "      <td>Natural language processing</td>\n",
       "      <td>natural language processing ( NLP ) be an inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Arabian Sea (Arabic:  , romanized: Al-Bahr...</td>\n",
       "      <td>Arabian Sea</td>\n",
       "      <td>the Arabian Sea ( Arabic :   , romanize : Al -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Suez Canal (Egyptian Arabic:  , Qanāt el S...</td>\n",
       "      <td>Suez Canal</td>\n",
       "      <td>the Suez Canal ( Egyptian Arabic :   , Qanāt e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Statistics (from German: Statistik, orig. \"des...</td>\n",
       "      <td>Statistics</td>\n",
       "      <td>statistic ( from german : Statistik , orig . \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dependent and independent variables are variab...</td>\n",
       "      <td>Dependent and independent variables</td>\n",
       "      <td>dependent and independent variable be variable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Gulf of Aden (Arabic:  , Somali: Gacanka C...</td>\n",
       "      <td>Gulf of Aden</td>\n",
       "      <td>the Gulf of Aden ( Arabic :   , Somali : Gacan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Machine learning (ML) is a field devoted to un...</td>\n",
       "      <td>Machine learning</td>\n",
       "      <td>machine learning ( ML ) be a field devote to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The European Central Bank (ECB) is the prime c...</td>\n",
       "      <td>European Central Bank</td>\n",
       "      <td>the European Central Bank ( ECB ) be the prime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A bank is a financial institution that accepts...</td>\n",
       "      <td>Bank</td>\n",
       "      <td>a bank be a financial institution that accept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fintech, a portmanteau of \"financial technolog...</td>\n",
       "      <td>Fintech</td>\n",
       "      <td>Fintech , a portmanteau of \" financial technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The International Monetary Fund (IMF) is a maj...</td>\n",
       "      <td>International Monetary Fund</td>\n",
       "      <td>the International Monetary Fund ( IMF ) be a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Basketball is a team sport in which two teams,...</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>basketball be a team sport in which two team ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Swimming is the self-propulsion of a person th...</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>swimming be the self - propulsion of a person ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tennis is a racket sport that is played either...</td>\n",
       "      <td>Tennis</td>\n",
       "      <td>Tennis be a racket sport that be play either i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              article  \\\n",
       "0   A desk pad or blotter is a table protector use...   \n",
       "1   Data science is an interdisciplinary academic ...   \n",
       "2   Artificial intelligence (AI) is intelligencepe...   \n",
       "3   Somaliland, officially the Republic of Somalil...   \n",
       "4   Natural language processing (NLP) is an interd...   \n",
       "5   The Arabian Sea (Arabic:  , romanized: Al-Bahr...   \n",
       "6   The Suez Canal (Egyptian Arabic:  , Qanāt el S...   \n",
       "7   Statistics (from German: Statistik, orig. \"des...   \n",
       "8   Dependent and independent variables are variab...   \n",
       "9   The Gulf of Aden (Arabic:  , Somali: Gacanka C...   \n",
       "10  Machine learning (ML) is a field devoted to un...   \n",
       "11  The European Central Bank (ECB) is the prime c...   \n",
       "12  A bank is a financial institution that accepts...   \n",
       "13  Fintech, a portmanteau of \"financial technolog...   \n",
       "14  The International Monetary Fund (IMF) is a maj...   \n",
       "15  Basketball is a team sport in which two teams,...   \n",
       "16  Swimming is the self-propulsion of a person th...   \n",
       "17  Tennis is a racket sport that is played either...   \n",
       "\n",
       "                                  title  \\\n",
       "0                              Desk pad   \n",
       "1                          Data science   \n",
       "2               Artificial intelligence   \n",
       "3                            Somaliland   \n",
       "4           Natural language processing   \n",
       "5                           Arabian Sea   \n",
       "6                            Suez Canal   \n",
       "7                            Statistics   \n",
       "8   Dependent and independent variables   \n",
       "9                          Gulf of Aden   \n",
       "10                     Machine learning   \n",
       "11                European Central Bank   \n",
       "12                                 Bank   \n",
       "13                              Fintech   \n",
       "14          International Monetary Fund   \n",
       "15                           Basketball   \n",
       "16                             Swimming   \n",
       "17                               Tennis   \n",
       "\n",
       "                                                lemma  \n",
       "0   a desk pad or blotter be a table protector use...  \n",
       "1   datum science be an interdisciplinary academic...  \n",
       "2   artificial intelligence ( AI ) be intelligence...  \n",
       "3   Somaliland , officially the Republic of Somali...  \n",
       "4   natural language processing ( NLP ) be an inte...  \n",
       "5   the Arabian Sea ( Arabic :   , romanize : Al -...  \n",
       "6   the Suez Canal ( Egyptian Arabic :   , Qanāt e...  \n",
       "7   statistic ( from german : Statistik , orig . \"...  \n",
       "8   dependent and independent variable be variable...  \n",
       "9   the Gulf of Aden ( Arabic :   , Somali : Gacan...  \n",
       "10  machine learning ( ML ) be a field devote to u...  \n",
       "11  the European Central Bank ( ECB ) be the prime...  \n",
       "12  a bank be a financial institution that accept ...  \n",
       "13  Fintech , a portmanteau of \" financial technol...  \n",
       "14  the International Monetary Fund ( IMF ) be a m...  \n",
       "15  basketball be a team sport in which two team ,...  \n",
       "16  swimming be the self - propulsion of a person ...  \n",
       "17  Tennis be a racket sport that be play either i...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if FILTER and TOKENIZER:\n",
    "    # remove numbers\n",
    "    corpus['lemma'] = corpus['lemma'].replace(r'\\b[0-9.].*?\\b', '', regex = True)#corpus\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfff2ef3-c745-438b-9269-5d725f284705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106806"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of words (by counting spaces)\n",
    "corpus['article'].str.count(' ').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fe268-ed3e-413b-abfa-b4206ed67f81",
   "metadata": {},
   "source": [
    "\n",
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b1ab9c-1029-44cd-b463-1585c7c6a2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9041, 18)\n",
      "CPU times: user 159 ms, sys: 0 ns, total: 159 ms\n",
      "Wall time: 159 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a2</th>\n",
       "      <td>0.067623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>0.067623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaai</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aateye</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aau</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>études</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>être</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>österreichischer</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>čapek</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>šarūnas</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9041 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0    1         2         3    4    5         6    7   \\\n",
       "a2                0.067623  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.0   \n",
       "a3                0.067623  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.0   \n",
       "aaai              0.000000  0.0  0.005345  0.000000  0.0  0.0  0.000000  0.0   \n",
       "aateye            0.000000  0.0  0.000000  0.003877  0.0  0.0  0.000000  0.0   \n",
       "aau               0.000000  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.0   \n",
       "...                    ...  ...       ...       ...  ...  ...       ...  ...   \n",
       "études            0.000000  0.0  0.000000  0.000000  0.0  0.0  0.004016  0.0   \n",
       "être              0.000000  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.0   \n",
       "österreichischer  0.000000  0.0  0.000000  0.000000  0.0  0.0  0.004016  0.0   \n",
       "čapek             0.000000  0.0  0.012214  0.000000  0.0  0.0  0.000000  0.0   \n",
       "šarūnas           0.000000  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.0   \n",
       "\n",
       "                   8    9         10        11   12   13   14        15   16  \\\n",
       "a2                0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "a3                0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "aaai              0.0  0.0  0.005262  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "aateye            0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "aau               0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.003764  0.0   \n",
       "...               ...  ...       ...       ...  ...  ...  ...       ...  ...   \n",
       "études            0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "être              0.0  0.0  0.000000  0.004117  0.0  0.0  0.0  0.000000  0.0   \n",
       "österreichischer  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "čapek             0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "šarūnas           0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.003764  0.0   \n",
       "\n",
       "                   17  \n",
       "a2                0.0  \n",
       "a3                0.0  \n",
       "aaai              0.0  \n",
       "aateye            0.0  \n",
       "aau               0.0  \n",
       "...               ...  \n",
       "études            0.0  \n",
       "être              0.0  \n",
       "österreichischer  0.0  \n",
       "čapek             0.0  \n",
       "šarūnas           0.0  \n",
       "\n",
       "[9041 rows x 18 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cutting tfidf: the larger the more common and stop words\n",
    "SPECF = 0.9\n",
    "\n",
    "if TOKENIZER:\n",
    "    docs= corpus['lemma'].tolist()\n",
    "else:\n",
    "    docs= corpus['article'].tolist()\n",
    "\n",
    "# Create Term-Document Matrix with TF-IDF weighting\n",
    "vectorizer = TfidfVectorizer(max_df = SPECF) # , norm=None)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(X.T.toarray(), index=vectorizer.get_feature_names_out())\n",
    "print(df.shape)\n",
    "df.tail()\n",
    "df.iloc[3000:3020,0:5]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fca385-4007-4bc6-919f-4959d21d03b6",
   "metadata": {},
   "source": [
    "### Save Index data\n",
    "\n",
    "* Save model data\n",
    "* Save tf-idf matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618d7341-de9f-4f41-a6f1-ab9b12f79663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: LANG: en, SPECF: 0.9, TOKENIZER: True, FILTER: True, tfidf-size: (9041, 18)\n",
      "(not saved)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "SAVEINDEX = False\n",
    "\n",
    "if LANG == 'en':\n",
    "    # model\n",
    "    fn_mod = \"data/simple_search.pkl\"\n",
    "    # tfidf matrix\n",
    "    fn_dfidf = \"data/simple_search.csv\"\n",
    "\n",
    "elif LANG == 'de':\n",
    "    # model\n",
    "    fn_mod = \"data/simple_search_de.pkl\"\n",
    "    # tfidf matrix\n",
    "    fn_dfidf = \"data/simple_search_de.csv\"\n",
    "\n",
    "# document addresses\n",
    "#fn_corpus = \"data/simle_search_corp.csv\"\n",
    "\n",
    "print(f'Parameter: LANG: {LANG}, SPECF: {SPECF}, TOKENIZER: {TOKENIZER}, FILTER: {FILTER}, tfidf-size: {df.shape}')\n",
    "\n",
    "if SAVEINDEX:\n",
    "    joblib.dump(vectorizer, fn_mod) \n",
    "    df.to_csv(fn_dfidf)\n",
    "    print(f'Saved: {fn_mod} and {fn_dfidf}')\n",
    "else:\n",
    "    print('(not saved)')\n",
    "        \n",
    "#corpusDst[[\"title\"]].to_csv(fn_corpus, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c58b9-6835-430d-85e1-bfc489e2424e",
   "metadata": {},
   "source": [
    "## Part 2: Query data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b64084-b48c-41bb-be19-5e9df553860c",
   "metadata": {},
   "source": [
    "### Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "161b8b79-f36d-464c-b23b-c59376cb2638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acc86502-21b5-48bc-b2d5-6b56d45f1e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LANG = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0eb275-9852-42cd-8c71-98eb89e2db80",
   "metadata": {},
   "source": [
    "Load model data and tfidf-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4924536-0649-46e6-8c5f-8520ebc3f864",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rp/.local/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rp/.local/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load trained model, ie ifidf matrix\n",
    "\n",
    "if LANG == 'en':\n",
    "    # model\n",
    "    fn_mod = \"data/simple_search.pkl\"\n",
    "    # tfidf matrix\n",
    "    fn_dfidf = \"data/simple_search.csv\"\n",
    "    # corpus; required if not only document number is needed but title and text as well\n",
    "    fn_corpus = 'data/corpus.csv'\n",
    "    \n",
    "elif LANG == 'de':\n",
    "    # model\n",
    "    fn_mod = \"data/simple_search_de.pkl\"\n",
    "    # tfidf matrix\n",
    "    fn_dfidf = \"data/simple_search_de.csv\"\n",
    "    # corpus\n",
    "    fn_corpus = 'data/corpus_de.csv'\n",
    "\n",
    "vectorizer = joblib.load(fn_mod)\n",
    "tfidf = pd.read_csv(fn_dfidf, index_col=0)\n",
    "corpus = pd.read_csv(fn_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4ca852b-a493-4db2-9e50-f6d6cfb0c733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.029503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018911</td>\n",
       "      <td>0.023593</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.013736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>études</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>être</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>österreichischer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>čapek</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>šarūnas</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9528 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0    1         2         3    4         5         6  \\\n",
       "00                0.0  0.0  0.000000  0.003040  0.0  0.000000  0.003125   \n",
       "000               0.0  0.0  0.005157  0.029503  0.0  0.018911  0.023593   \n",
       "0001              0.0  0.0  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "0003              0.0  0.0  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "001               0.0  0.0  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "...               ...  ...       ...       ...  ...       ...       ...   \n",
       "études            0.0  0.0  0.000000  0.000000  0.0  0.000000  0.003972   \n",
       "être              0.0  0.0  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "österreichischer  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.003972   \n",
       "čapek             0.0  0.0  0.012154  0.000000  0.0  0.000000  0.000000   \n",
       "šarūnas           0.0  0.0  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "                        7    8         9        10        11        12  \\\n",
       "00                0.00000  0.0  0.000000  0.000000  0.003237  0.000000   \n",
       "000               0.00000  0.0  0.009997  0.005098  0.001745  0.014489   \n",
       "0001              0.00000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "0003              0.00727  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "001               0.00000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "...                   ...  ...       ...       ...       ...       ...   \n",
       "études            0.00000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "être              0.00000  0.0  0.000000  0.000000  0.004114  0.000000   \n",
       "österreichischer  0.00000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "čapek             0.00000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "šarūnas           0.00000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                        13        14        15        16        17  \n",
       "00                0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "000               0.007198  0.003625  0.000000  0.006556  0.013736  \n",
       "0001              0.000000  0.004272  0.000000  0.000000  0.000000  \n",
       "0003              0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "001               0.000000  0.004272  0.000000  0.000000  0.000000  \n",
       "...                    ...       ...       ...       ...       ...  \n",
       "études            0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "être              0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "österreichischer  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "čapek             0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "šarūnas           0.000000  0.000000  0.003756  0.000000  0.000000  \n",
       "\n",
       "[9528 rows x 18 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affc4a3-6cdd-49b3-805c-e6d073bd707d",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c37ba43-1676-478e-8d99-70efb3e45f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.08644678929441071),\n",
       " (6, 0.0197867104200396),\n",
       " (3, 0.011023250932349729),\n",
       " (7, 0.0038684275756352485),\n",
       " (17, 0.0),\n",
       " (1, 0.0),\n",
       " (2, 0.0),\n",
       " (4, 0.0),\n",
       " (5, 0.0),\n",
       " (8, 0.0),\n",
       " (16, 0.0),\n",
       " (10, 0.0),\n",
       " (11, 0.0),\n",
       " (12, 0.0),\n",
       " (13, 0.0),\n",
       " (14, 0.0),\n",
       " (15, 0.0),\n",
       " (0, 0.0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw demo\n",
    "#q = ['djibouti waterway']\n",
    "#q = ['has djibouti a waterway']\n",
    "q = ['sag mir alles, was du über den waterway in djibouti weisst']\n",
    "\n",
    "q_vec = vectorizer.transform(q)\n",
    "search_results = (q_vec @ tfidf)[0]\n",
    "si = search_results.argsort().tolist()[::-1]\n",
    "list(zip(si, search_results[si]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179ed08-ba6e-48a1-92ad-13fa1cda5b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44f69ef0-aeff-4e95-a394-9f83485aa807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_articles(q, df, vec, verbose=False): \n",
    "    \"\"\"\n",
    "    Find documents which fit to query. Higher ranking when more token fit to query (\"OR\"-query)\n",
    "    \n",
    "    input:\n",
    "    q: query string\n",
    "    df: tfidf matrix\n",
    "    vec: tfidf transformer\n",
    "    output:\n",
    "    (document index, weight)-vector\n",
    "    \n",
    "    convert query q into tfidf-vector; calculate query-vector @ tfidf-matrix    \n",
    "    \"\"\"\n",
    "    q = [' '.join(q)]\n",
    "    if verbose:   print('q OR: ', q)\n",
    "    q_vec = vec.transform(q) # .toarray().reshape(df.shape[0],)\n",
    "    search_results = (q_vec @ df)[0]\n",
    "    si = search_results.argsort().tolist()[::-1]\n",
    "    return list(zip(si, search_results[si]))\n",
    "\n",
    "def get_similar_articles_and(q_and, df, vec, verbose = False): \n",
    "    \"\"\"\n",
    "    Find documents which answer query. \n",
    "    All token must be present in result documents (\"AND\"-query). No result if query contains common or \"stop\" words.\n",
    "    \n",
    "    input:\n",
    "    q: query string\n",
    "    df: tfidf matrix\n",
    "    vec: tfidf transformer\n",
    "    output:\n",
    "    (document index, weight)-vector\n",
    "    \n",
    "    Convert query q into tfidf-vector; calculate query-vector @ tfidf-matrix.\n",
    "    Handle each element of querystring separately and multiply the results.\n",
    "    Documents missing a query element has weight of null, same with product of results.\n",
    "    \"\"\"\n",
    "\n",
    "    search_results = np.ones(df.shape[1]).reshape(-1,)\n",
    "    #q_list = q_and.split()\n",
    "    q_list = q_and\n",
    "    for qi in q_list:\n",
    "        q = [qi]\n",
    "        if verbose:  print('qi AND: ', q)\n",
    "        q_vec = vec.transform(q) # .toarray().reshape(df.shape[0],)\n",
    "        resv = (q_vec @ df)[0]\n",
    "        resv_norm = np.linalg.norm(resv)\n",
    "        if resv_norm < 1e-5: \n",
    "            resv_norm = 1e10 \n",
    "        search_results *= resv /resv_norm\n",
    "    si = search_results.argsort().tolist()[::-1]\n",
    "    return list(zip(si, search_results[si]))\n",
    "\n",
    "\n",
    "\n",
    "def process_result(sim_sorted):\n",
    "    \"\"\"\n",
    "    nice output of results\n",
    "    input: dictionary{document_index: similarity}\n",
    "        print(index, similarity, document title)\n",
    "    \"\"\"\n",
    "    global corpus\n",
    "\n",
    "    for k, v in sim_sorted:\n",
    "        if v != 0.0:\n",
    "            print(f\"Weight: {v:.5f} in {k:2d}:  {corpus.iloc[k,1]}\")\n",
    "    return None\n",
    "            \n",
    "def answer_question(q):\n",
    "    \"\"\"\n",
    "    process query string and print answer\n",
    "    input: query string\n",
    "    output: none\n",
    "    print answer via function process_results\n",
    "    \"\"\"\n",
    "    q_list = q.lower().split()\n",
    "    searchlist=[]\n",
    "    for qi in q_list:\n",
    "        if '*' in qi: \n",
    "            qiq = qi.replace('*', '.*')\n",
    "            searchlist.append(' '.join(list(tfidf[tfidf.index.str.match(fr'{qiq}') == True].index)))\n",
    "        else:\n",
    "            #print(qi)\n",
    "            searchlist.append(qi)\n",
    "    return searchlist\n",
    "\n",
    "def demo_query(q, verbose = False):\n",
    "    \"\"\"\n",
    "    Print statistical data for query q\n",
    "\n",
    "    input: query string\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Query: \", q)\n",
    "\n",
    "    q_new = answer_question(q)\n",
    "    if verbose: print(q_new)\n",
    "    print('AND')\n",
    "    process_result(get_similar_articles_and(q_new, tfidf, vectorizer, verbose))\n",
    "    print('OR')\n",
    "    process_result(get_similar_articles(q_new, tfidf, vectorizer, verbose))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "431b6b5f-f6f1-4d2c-9e5a-1d694dd3a644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Djibouti Erythraean\n",
      "AND\n",
      "Weight: 0.74604 in  9:  Gulf of Aden\n",
      "OR\n",
      "Weight: 0.11667 in  9:  Gulf of Aden\n",
      "Weight: 0.03678 in  5:  Arabian Sea\n",
      "Weight: 0.01674 in  3:  Somaliland\n",
      "\n",
      "Query:   djibouti waterway\n",
      "AND\n",
      "Weight: 0.93483 in  9:  Gulf of Aden\n",
      "OR\n",
      "Weight: 0.13125 in  9:  Gulf of Aden\n",
      "Weight: 0.01721 in  6:  Suez Canal\n",
      "Weight: 0.01674 in  3:  Somaliland\n",
      "\n",
      "Query:   djibouti water*\n",
      "AND\n",
      "Weight: 0.37546 in  9:  Gulf of Aden\n",
      "Weight: 0.01494 in  3:  Somaliland\n",
      "OR\n",
      "Weight: 0.09124 in 16:  Swimming\n",
      "Weight: 0.07327 in  9:  Gulf of Aden\n",
      "Weight: 0.01730 in  6:  Suez Canal\n",
      "Weight: 0.01462 in  3:  Somaliland\n",
      "Weight: 0.00987 in  5:  Arabian Sea\n",
      "\n",
      "Query:   Har*sa Soma*\n",
      "AND\n",
      "Weight: 0.99510 in  3:  Somaliland\n",
      "OR\n",
      "Weight: 0.39443 in  3:  Somaliland\n",
      "Weight: 0.03402 in  9:  Gulf of Aden\n",
      "Weight: 0.00810 in  5:  Arabian Sea\n",
      "Weight: 0.00072 in  6:  Suez Canal\n",
      "\n",
      "Query:  djibo* eryth* \n",
      "AND\n",
      "Weight: 0.82931 in  9:  Gulf of Aden\n",
      "OR\n",
      "Weight: 0.07890 in  9:  Gulf of Aden\n",
      "Weight: 0.02139 in  5:  Arabian Sea\n",
      "Weight: 0.01155 in  3:  Somaliland\n",
      "Weight: 0.00373 in  6:  Suez Canal\n",
      "\n",
      "Query:  tell me more about Somalia\n",
      "AND\n",
      "OR\n",
      "Weight: 0.08098 in  3:  Somaliland\n",
      "Weight: 0.02643 in 13:  Fintech\n",
      "Weight: 0.02583 in  4:  Natural language processing\n",
      "Weight: 0.02577 in  2:  Artificial intelligence\n",
      "Weight: 0.02507 in  5:  Arabian Sea\n",
      "Weight: 0.02406 in  9:  Gulf of Aden\n",
      "Weight: 0.01773 in 10:  Machine learning\n",
      "Weight: 0.01768 in  7:  Statistics\n",
      "Weight: 0.01677 in  6:  Suez Canal\n",
      "Weight: 0.01464 in 14:  International Monetary Fund\n",
      "Weight: 0.01244 in 17:  Tennis\n",
      "Weight: 0.01064 in 12:  Bank\n",
      "Weight: 0.00975 in 15:  Basketball\n",
      "Weight: 0.00940 in 11:  European Central Bank\n",
      "Weight: 0.00787 in  1:  Data science\n",
      "Weight: 0.00722 in 16:  Swimming\n",
      "Weight: 0.00212 in  8:  Dependent and independent variables\n",
      "\n",
      "******************************\n",
      "verbose: True:\n",
      "Query:  Djibouti Erythraean\n",
      "['djibouti', 'erythraean']\n",
      "AND\n",
      "qi AND:  ['djibouti']\n",
      "qi AND:  ['erythraean']\n",
      "Weight: 0.74604 in  9:  Gulf of Aden\n",
      "OR\n",
      "q OR:  ['djibouti erythraean']\n",
      "Weight: 0.11667 in  9:  Gulf of Aden\n",
      "Weight: 0.03678 in  5:  Arabian Sea\n",
      "Weight: 0.01674 in  3:  Somaliland\n",
      "\n",
      "Query:   djibouti waterway\n",
      "['djibouti', 'waterway']\n",
      "AND\n",
      "qi AND:  ['djibouti']\n",
      "qi AND:  ['waterway']\n",
      "Weight: 0.93483 in  9:  Gulf of Aden\n",
      "OR\n",
      "q OR:  ['djibouti waterway']\n",
      "Weight: 0.13125 in  9:  Gulf of Aden\n",
      "Weight: 0.01721 in  6:  Suez Canal\n",
      "Weight: 0.01674 in  3:  Somaliland\n",
      "\n",
      "Query:   djibouti water*\n",
      "['djibouti', 'water watercourse waterfront wateror waterpark watertight waterway']\n",
      "AND\n",
      "qi AND:  ['djibouti']\n",
      "qi AND:  ['water watercourse waterfront wateror waterpark watertight waterway']\n",
      "Weight: 0.37546 in  9:  Gulf of Aden\n",
      "Weight: 0.01494 in  3:  Somaliland\n",
      "OR\n",
      "q OR:  ['djibouti water watercourse waterfront wateror waterpark watertight waterway']\n",
      "Weight: 0.09124 in 16:  Swimming\n",
      "Weight: 0.07327 in  9:  Gulf of Aden\n",
      "Weight: 0.01730 in  6:  Suez Canal\n",
      "Weight: 0.01462 in  3:  Somaliland\n",
      "Weight: 0.00987 in  5:  Arabian Sea\n",
      "\n",
      "Query:   Har*sa Soma*\n",
      "['hargeisa hargeisan', 'somali somalia somalian somaliland somalilander somalilanders somalis']\n",
      "AND\n",
      "qi AND:  ['hargeisa hargeisan']\n",
      "qi AND:  ['somali somalia somalian somaliland somalilander somalilanders somalis']\n",
      "Weight: 0.99510 in  3:  Somaliland\n",
      "OR\n",
      "q OR:  ['hargeisa hargeisan somali somalia somalian somaliland somalilander somalilanders somalis']\n",
      "Weight: 0.39443 in  3:  Somaliland\n",
      "Weight: 0.03402 in  9:  Gulf of Aden\n",
      "Weight: 0.00810 in  5:  Arabian Sea\n",
      "Weight: 0.00072 in  6:  Suez Canal\n",
      "\n",
      "Query:  djibo* eryth* \n",
      "['djibouti djiboutian', 'erythraean erythrean erythrà']\n",
      "AND\n",
      "qi AND:  ['djibouti djiboutian']\n",
      "qi AND:  ['erythraean erythrean erythrà']\n",
      "Weight: 0.82931 in  9:  Gulf of Aden\n",
      "OR\n",
      "q OR:  ['djibouti djiboutian erythraean erythrean erythrà']\n",
      "Weight: 0.07890 in  9:  Gulf of Aden\n",
      "Weight: 0.02139 in  5:  Arabian Sea\n",
      "Weight: 0.01155 in  3:  Somaliland\n",
      "Weight: 0.00373 in  6:  Suez Canal\n",
      "\n",
      "Query:  tell me more about Somalia\n",
      "['tell', 'me', 'more', 'about', 'somalia']\n",
      "AND\n",
      "qi AND:  ['tell']\n",
      "qi AND:  ['me']\n",
      "qi AND:  ['more']\n",
      "qi AND:  ['about']\n",
      "qi AND:  ['somalia']\n",
      "OR\n",
      "q OR:  ['tell me more about somalia']\n",
      "Weight: 0.08098 in  3:  Somaliland\n",
      "Weight: 0.02643 in 13:  Fintech\n",
      "Weight: 0.02583 in  4:  Natural language processing\n",
      "Weight: 0.02577 in  2:  Artificial intelligence\n",
      "Weight: 0.02507 in  5:  Arabian Sea\n",
      "Weight: 0.02406 in  9:  Gulf of Aden\n",
      "Weight: 0.01773 in 10:  Machine learning\n",
      "Weight: 0.01768 in  7:  Statistics\n",
      "Weight: 0.01677 in  6:  Suez Canal\n",
      "Weight: 0.01464 in 14:  International Monetary Fund\n",
      "Weight: 0.01244 in 17:  Tennis\n",
      "Weight: 0.01064 in 12:  Bank\n",
      "Weight: 0.00975 in 15:  Basketball\n",
      "Weight: 0.00940 in 11:  European Central Bank\n",
      "Weight: 0.00787 in  1:  Data science\n",
      "Weight: 0.00722 in 16:  Swimming\n",
      "Weight: 0.00212 in  8:  Dependent and independent variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "querylist = [ 'Djibouti Erythraean',  ' djibouti waterway', ' djibouti water*', \n",
    "             ' Har*sa Soma*', 'djibo* eryth* ',\n",
    "             'tell me more about Somalia']\n",
    "\n",
    "for q in querylist:\n",
    "    demo_query(q, False)\n",
    "\n",
    "print('*'*30)\n",
    "print('verbose: True:')\n",
    "for q in querylist:\n",
    "    demo_query(q, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926068-11f1-40d5-801a-7f7949a8d3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
